from fastapi import FastAPI, APIRouter, UploadFile, File, HTTPException
from fastapi.responses import StreamingResponse
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional
import uuid
from datetime import datetime, timezone
import tempfile
import re
import math
import zipfile
import rarfile
import io

# PDF manipulation
from PyPDF2 import PdfReader, PdfWriter

# Import Emergent LLM integration
from emergentintegrations.llm.chat import LlmChat, UserMessage, FileContentWithMimeType

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection
mongo_url = os.environ.get('MONGO_URL')
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ.get('DB_NAME')]

# Emergent LLM Key
EMERGENT_LLM_KEY = os.environ.get('EMERGENT_LLM_KEY')

# Limite de taille pour Gemini (serveurs limit√©s √† 20 Mo)
# Optimis√© √† 7 Mo pour √©quilibre entre fluidit√© et nombre de segments
MAX_CHUNK_SIZE = 7 * 1024 * 1024  # 7 Mo pour √©quilibre optimal
MAX_PAGES_PER_CHUNK = 12  # Maximum 12 pages par segment pour √©viter timeouts

# R√©pertoire pour les fichiers temporaires
UPLOAD_DIR = tempfile.gettempdir()

# Create the main app
app = FastAPI(title="L'√âclaireur API", description="Outil d'aide pour les travailleurs qu√©b√©cois")

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Models
class AnalysisResult(BaseModel):
    model_config = ConfigDict(extra="ignore")
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    filename: str
    file_size: int
    analysis: str
    anonymized_analysis: str
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class AnalysisResponse(BaseModel):
    success: bool
    filename: str
    file_size: int
    analysis: str
    anonymized_for_ai: str  # Version anonymis√©e pour l'apprentissage IA
    message: str
    segments_analyzed: int = 1
    destruction_confirmed: bool = True

# Mod√®les pour les fiches m√©decins
class MedecinCreate(BaseModel):
    nom: str = Field(..., min_length=2, max_length=100)
    prenom: str = Field(..., min_length=2, max_length=100)
    specialite: Optional[str] = None
    adresse: Optional[str] = None
    ville: Optional[str] = None
    diplomes: Optional[str] = None
    source_info: Optional[str] = None

class MedecinStats(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    nom: str
    prenom: str
    specialite: Optional[str] = None
    adresse: Optional[str] = None
    ville: Optional[str] = None
    diplomes: Optional[str] = None
    decisions_pro_employeur: int = 0
    decisions_pro_employe: int = 0
    total_decisions: int = 0
    pourcentage_pro_employeur: float = 0.0
    pourcentage_pro_employe: float = 0.0
    sources: List[str] = []
    derniere_maj: str = ""

# Mod√®le pour les contributions utilisateurs
class ContributionCreate(BaseModel):
    medecin_nom: str = Field(..., min_length=2, max_length=100)
    medecin_prenom: str = Field(..., min_length=2, max_length=100)
    type_contribution: str = Field(..., pattern="^(pro_employeur|pro_employe|info_generale)$")
    description: str = Field(..., min_length=20, max_length=2000)
    source_reference: Optional[str] = None

# Liste de mots interdits pour la mod√©ration
MOTS_INTERDITS = [
    "con", "connard", "connasse", "merde", "putain", "salaud", "salope", "encul√©",
    "fuck", "shit", "bitch", "asshole", "bastard",
    "n√®gre", "n√©gro", "arabe", "sale", "terroriste", "islamiste",
    "p√©d√©", "tapette", "gouine", "travelo",
    "tuer", "mort", "crever", "buter", "assassin", "violence",
    "frapper", "tabasser", "lyncher",
    "menace", "revenge", "vengeance", "payer cher",
]

def moderer_contenu(texte: str) -> tuple[bool, str]:
    """V√©rifie si le contenu contient des termes interdits."""
    texte_lower = texte.lower()
    for mot in MOTS_INTERDITS:
        if mot in texte_lower:
            return False, f"Contenu inappropri√© d√©tect√©. Merci de reformuler de mani√®re factuelle et respectueuse."
    return True, ""

# ===== DESTRUCTION S√âCURIS√âE DOD 5220.22-M =====
def destruction_securisee(chemin_fichier: str) -> bool:
    """
    Destruction s√©curis√©e du fichier selon les standards DOD 5220.22-M
    3 passes: z√©ros, uns, donn√©es al√©atoires
    """
    try:
        if os.path.exists(chemin_fichier):
            taille = os.path.getsize(chemin_fichier)
            # Pass 1: √âcriture de z√©ros
            with open(chemin_fichier, 'wb') as f:
                f.write(b'\x00' * taille)
                f.flush()
                os.fsync(f.fileno())
            # Pass 2: √âcriture de uns (0xFF)
            with open(chemin_fichier, 'wb') as f:
                f.write(b'\xFF' * taille)
                f.flush()
                os.fsync(f.fileno())
            # Pass 3: √âcriture de donn√©es al√©atoires
            with open(chemin_fichier, 'wb') as f:
                f.write(os.urandom(taille))
                f.flush()
                os.fsync(f.fileno())
            # Suppression finale
            os.remove(chemin_fichier)
            logger.info(f"Fichier d√©truit de mani√®re s√©curis√©e (DOD 5220.22-M): {chemin_fichier}")
            return True
        return False
    except Exception as e:
        logger.error(f"Erreur lors de la destruction s√©curis√©e: {str(e)}")
        # En cas d'erreur, tenter une suppression simple
        if os.path.exists(chemin_fichier):
            os.remove(chemin_fichier)
        return False

# ===== ANONYMISATION =====
def anonymize_for_report(text: str) -> str:
    """
    Anonymise uniquement les donn√©es ultra-sensibles pour le rapport t√©l√©chargeable.
    GARDE: noms, t√©l√©phones, adresses
    MASQUE: NAS, RAMQ, Permis, Coordonn√©es bancaires
    """
    # NAS (format: XXX-XXX-XXX ou XXX XXX XXX ou XXXXXXXXX)
    text = re.sub(r'\b\d{3}[-\s]?\d{3}[-\s]?\d{3}\b', '[NAS masqu√©]', text)
    
    # RAMQ (format: XXXX XXXX XXXX ou 12 caract√®res alphanum√©riques)
    text = re.sub(r'\b[A-Z]{4}\s?\d{4}\s?\d{4}\b', '[RAMQ masqu√©]', text, flags=re.IGNORECASE)
    text = re.sub(r'\b[A-Z]{4}\d{8}\b', '[RAMQ masqu√©]', text, flags=re.IGNORECASE)
    
    # Permis de conduire (format variable qu√©b√©cois)
    text = re.sub(r'\b[A-Z]\d{4}[-\s]?\d{5}[-\s]?\d{2}\b', '[Permis masqu√©]', text, flags=re.IGNORECASE)
    
    # Coordonn√©es bancaires (num√©ros de compte, transit, etc.)
    text = re.sub(r'\b\d{5}[-\s]?\d{3}[-\s]?\d{7}\b', '[Info bancaire masqu√©e]', text)  # Transit-Institution-Compte
    text = re.sub(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b', '[Info bancaire masqu√©e]', text)  # Carte de cr√©dit
    
    # Num√©ros de dossier CNESST (on garde car utile)
    
    return text

def anonymize_for_ai_learning(text: str) -> str:
    """
    Anonymisation COMPL√àTE pour l'apprentissage IA.
    MASQUE TOUT: noms, t√©l√©phones, adresses, NAS, RAMQ, etc.
    """
    # D'abord appliquer l'anonymisation de base
    text = anonymize_for_report(text)
    
    # Ensuite anonymiser le reste pour l'IA
    
    # Num√©ros de t√©l√©phone
    text = re.sub(r'\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b', '[T√âL masqu√©]', text)
    text = re.sub(r'\b\(\d{3}\)\s?\d{3}[-.\s]?\d{4}\b', '[T√âL masqu√©]', text)
    
    # Codes postaux canadiens
    text = re.sub(r'\b[A-Za-z]\d[A-Za-z][-\s]?\d[A-Za-z]\d\b', '[CODE POSTAL masqu√©]', text)
    
    # Adresses courriel
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[COURRIEL masqu√©]', text)
    
    # Adresses (patterns courants)
    text = re.sub(r'\b\d{1,5}\s+(?:rue|avenue|boulevard|chemin|place|rang|route|c√¥te)\s+[A-Za-z√Ä-√ø\s\-]+', '[ADRESSE masqu√©e]', text, flags=re.IGNORECASE)
    
    # Noms propres apr√®s Dr, Me, M., Mme (approximatif)
    text = re.sub(r'\b(Dr|Me|M\.|Mme|Mr)\s+([A-Z][a-z√†-√ø]+)\s+([A-Z][a-z√†-√ø]+)', r'\1 [NOM masqu√©]', text)
    
    return text

def split_pdf_into_chunks(pdf_path: str, max_size_bytes: int = MAX_CHUNK_SIZE) -> List[str]:
    """Divise un PDF volumineux en plusieurs fichiers plus petits pour √©viter les timeouts Gemini."""
    chunk_paths = []
    
    try:
        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)
        
        if total_pages == 0:
            return [pdf_path]
        
        file_size = os.path.getsize(pdf_path)
        avg_page_size = file_size / total_pages
        
        # Calculer pages par chunk bas√© sur la taille
        pages_per_chunk = max(1, int(max_size_bytes / avg_page_size))
        # Limiter strictement √† MAX_PAGES_PER_CHUNK pour √©viter timeouts Gemini
        pages_per_chunk = min(pages_per_chunk, MAX_PAGES_PER_CHUNK)
        
        num_chunks = math.ceil(total_pages / pages_per_chunk)
        
        logger.info(f"PDF de {total_pages} pages ({file_size/(1024*1024):.1f} Mo), divis√© en {num_chunks} segments de ~{pages_per_chunk} pages")
        
        for i in range(num_chunks):
            writer = PdfWriter()
            start_page = i * pages_per_chunk
            end_page = min((i + 1) * pages_per_chunk, total_pages)
            
            for page_num in range(start_page, end_page):
                writer.add_page(reader.pages[page_num])
            
            chunk_path = f"{pdf_path}_segment_{i+1}.pdf"
            with open(chunk_path, 'wb') as chunk_file:
                writer.write(chunk_file)
            
            chunk_paths.append(chunk_path)
            logger.info(f"Segment {i+1}/{num_chunks} cr√©√©: pages {start_page+1}-{end_page}")
        
        return chunk_paths
        
    except Exception as e:
        logger.error(f"Erreur lors de la segmentation du PDF: {str(e)}")
        return [pdf_path]

# ===== SYSTEM MESSAGE ENRICHI =====
SYSTEM_MESSAGE_ANALYSE = """Tu es un expert en analyse de documents de la CNESST et du TAT pour les travailleurs qu√©b√©cois accident√©s.

## R√àGLES D'ANONYMISATION (RAPPORT FINAL)
Tu dois MASQUER uniquement:
- **NAS**: Remplacer par `[NAS masqu√©]`
- **RAMQ**: Remplacer par `[RAMQ masqu√©]`
- **Permis de conduire**: Remplacer par `[Permis masqu√©]`
- **Coordonn√©es bancaires**: Remplacer par `[Info bancaire masqu√©e]`

Tu GARDES en clair: noms des parties, t√©l√©phones, adresses (car rapport destin√© au TAT/avocats).

## EXPLICATIONS DES TERMES TECHNIQUES
√Ä CHAQUE terme m√©dical ou technique, ajoute SYST√âMATIQUEMENT une explication entre parenth√®ses juste apr√®s.
Exemples:
- "st√©nose (r√©tr√©cissement)"
- "kyste synovial (poche de liquide articulaire)"
- "ligament crois√© ant√©rieur ou LCA (ligament stabilisateur du genou)"
- "IRM (imagerie par r√©sonance magn√©tique - examen sans radiation)"
- "√©panchement synovial (accumulation de liquide dans l'articulation)"

## BAR√àMES CNESST - INDEMNIT√âS POUR ATTEINTE PERMANENTE (2024)
| Blessure | % atteinte | Indemnit√© estim√©e |
|----------|------------|-------------------|
| Ligament crois√© ant√©rieur (LCA) | 2% √† 10% | 2,400$ √† 12,000$ |
| Ligament crois√© post√©rieur (LCP) | 2% √† 8% | 2,400$ √† 9,600$ |
| M√©nisque (m√©niscectomie partielle) | 1% √† 5% | 1,200$ √† 6,000$ |
| Hernie discale cervicale | 2% √† 15% | 2,400$ √† 18,000$ |
| Hernie discale lombaire | 2% √† 15% | 2,400$ √† 18,000$ |
| Syndrome du canal carpien | 1% √† 5% | 1,200$ √† 6,000$ |
| Tendinite chronique | 1% √† 3% | 1,200$ √† 3,600$ |
| Fracture vert√©brale | 5% √† 25% | 6,000$ √† 30,000$ |
| Kyste synovial/adh√©rent | 1% √† 5% | 1,200$ √† 6,000$ |
| Lombalgie chronique | 2% √† 10% | 2,400$ √† 12,000$ |
| TSPT | 5% √† 35% | 6,000$ √† 42,000$ |
(Base: environ 1,200$ par 1% d'atteinte permanente)

## D√âLAIS IMPORTANTS √Ä MENTIONNER
- **Contestation d√©cision CNESST**: 30 jours
- **Contestation au TAT**: 45 jours
- **R√©clamation initiale CNESST**: 6 mois apr√®s l'accident
- **R√©cidive, rechute ou aggravation**: Aucun d√©lai (mais agir rapidement)

## FORMAT DU RAPPORT

# üìã RAPPORT D'ANALYSE D√âFENSE - L'√âCLAIREUR

## 1. üìù R√âSUM√â DU DOSSIER
*Synth√®se avec explications des termes techniques entre parenth√®ses*

## 2. üìÖ CHRONOLOGIE D√âTAILL√âE
| Date | √âv√©nement | Document/Page | Importance |
|------|-----------|---------------|------------|

## 3. üî¨ PREUVES M√âDICALES OBJECTIVES
| Date | Type d'examen | R√©sultats | Page du dossier |
|------|---------------|-----------|-----------------|

## 4. üë®‚Äç‚öïÔ∏è M√âDECINS ET EXPERTS IDENTIFI√âS
| M√©decin | Sp√©cialit√©/Qualifications | Mandat√© par | Conclusion | Coh√©rence avec imagerie |
|---------|---------------------------|-------------|------------|------------------------|
*Pour chaque m√©decin, v√©rifier sur le Coll√®ge des m√©decins (cmq.org)*

## 5. ‚ö†Ô∏è ANALYSE CRITIQUE - INCOH√âRENCES D√âTECT√âES
Pour chaque incoh√©rence:
- **Expert**: Dr [Nom]
- **Affirmation**: [Ce qu'il dit]
- **Preuve contradictoire**: [R√©sultat objectif - Page X]
- **Impact pour le travailleur**: [Cons√©quence]

## 6. üí∞ BAR√àME INDEMNISATIONS APPLICABLE
| Blessure identifi√©e | Fourchette % | Indemnit√© estim√©e |
|---------------------|--------------|-------------------|

## 7. ‚ùì QUESTIONS STRAT√âGIQUES POUR L'AUDIENCE TAT
### Questions pour confronter les experts de l'employeur:
1. "Dr [Nom], avez-vous examin√© personnellement les images de l'IRM du [date]?"
2. "Comment expliquez-vous que votre conclusion diff√®re du r√©sultat objectif de [examen]?"
3. "Combien de temps avez-vous pass√© avec le travailleur?"
4. [Questions sp√©cifiques bas√©es sur les contradictions]

## 8. ‚è∞ D√âLAIS IMPORTANTS
- D√©lai de contestation: [X jours restants si applicable]
- Prochaine √©ch√©ance: [date]

## 9. üìå ACTIONS RECOMMAND√âES
1. [Action prioritaire avec d√©lai]
2. [Documents √† obtenir]
3. [Professionnels √† consulter]

---

## üìä TABLEAU R√âCAPITULATIF - SYNTH√àSE DES CONTRADICTIONS

**CE TABLEAU DOIT APPARA√éTRE EN FIN DE RAPPORT**

| # | Expert | Ce qu'il affirme | Preuve objective contradictoire | Page | Impact |
|---|--------|------------------|--------------------------------|------|--------|
| 1 | Dr [Nom] | [Affirmation] | [Preuve IRM/Radio/etc.] | p.XX | [Minimisation/Omission] |
| 2 | ... | ... | ... | ... | ... |

**L√©gende:**
- ‚ùå Contradiction majeure (preuve objective ignor√©e)
- ‚ö†Ô∏è Incoh√©rence notable (interpr√©tation discutable)
- ‚úÖ Position coh√©rente avec les preuves

---

## ‚öñÔ∏è AVERTISSEMENT L√âGAL

> Ce rapport est un OUTIL D'AIDE √Ä LA COMPR√âHENSION g√©n√©r√© par intelligence artificielle.
> Il NE CONSTITUE PAS un avis juridique ou m√©dical professionnel.
> Ce rapport est destin√© UNIQUEMENT au travailleur, son avocat, son repr√©sentant syndical, la CNESST et le TAT.
> Il ne doit PAS √™tre partag√© en dehors du cadre juridique du dossier.

**Ressources:**
- Aide juridique Qu√©bec: https://www.justice.gouv.qc.ca/aide-juridique/
- TAT: https://www.tat.gouv.qc.ca/
- CNESST: https://www.cnesst.gouv.qc.ca/
- Barreau du Qu√©bec: https://www.barreau.qc.ca/fr/trouver-avocat/
- Coll√®ge des m√©decins - D√©cisions disciplinaires: https://www.cmq.org/fr/proteger-le-public/suivre-dossier-disciplinaire/decisions-disciplinaires

---
*Rapport g√©n√©r√© par L'√âclaireur - Propuls√© par E1 (Emergent) et Google Gemini*
*Date d'analyse: {date_analyse}*
"""

async def analyze_pdf_segment(pdf_path: str, segment_num: int, total_segments: int, max_retries: int = 3) -> str:
    """Analyse un segment de PDF avec Gemini avec retry automatique optimis√©."""
    import asyncio
    
    date_analyse = datetime.now(timezone.utc).strftime("%d/%m/%Y √† %H:%M UTC")
    
    for attempt in range(max_retries):
        try:
            logger.info(f"Analyse segment {segment_num}/{total_segments} - tentative {attempt+1}/{max_retries}")
            
            chat = LlmChat(
                api_key=EMERGENT_LLM_KEY,
                session_id=f"analysis-{uuid.uuid4()}",
                system_message=SYSTEM_MESSAGE_ANALYSE.replace("{date_analyse}", date_analyse)
            ).with_model("gemini", "gemini-2.5-flash")
            
            pdf_file = FileContentWithMimeType(
                file_path=pdf_path,
                mime_type="application/pdf"
            )
            
            segment_info = ""
            if total_segments > 1:
                segment_info = f"\n\n[SEGMENT {segment_num}/{total_segments}]"
            
            user_message = UserMessage(
                text=f"""Analyse ce document{segment_info} et produis un RAPPORT COMPLET DE D√âFENSE.

RAPPELS CRITIQUES:
1. EXPLIQUE CHAQUE TERME M√âDICAL/TECHNIQUE entre parenth√®ses (ex: "st√©nose (r√©tr√©cissement)")
2. Indique les NUM√âROS DE PAGES quand tu cites des informations
3. Le TABLEAU R√âCAPITULATIF DES CONTRADICTIONS doit √™tre EN FIN DE RAPPORT
4. Inclus le BAR√àME DES INDEMNISATIONS applicable
5. Pr√©pare des QUESTIONS STRAT√âGIQUES pour l'audience TAT

ANONYMISATION - MASQUER uniquement:
- NAS ‚Üí [NAS masqu√©]
- RAMQ ‚Üí [RAMQ masqu√©]  
- Permis ‚Üí [Permis masqu√©]
- Coordonn√©es bancaires ‚Üí [Info bancaire masqu√©e]

GARDER EN CLAIR: noms, t√©l√©phones, adresses (rapport destin√© au TAT/avocats)

Le travailleur compte sur toi pour l'aider √† comprendre son dossier et se d√©fendre.""",
                file_contents=[pdf_file]
            )
            
            response = await chat.send_message(user_message)
            return response
            
        except Exception as e:
            error_str = str(e)
            logger.error(f"Erreur segment {segment_num}: {error_str[:200]}")
            if "502" in error_str or "503" in error_str or "timeout" in error_str.lower() or "500" in error_str:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # 5s, 10s, 15s - d√©lais r√©duits
                    logger.warning(f"Erreur temporaire segment {segment_num}, retry {attempt+2}/{max_retries} dans {wait_time}s...")
                    await asyncio.sleep(wait_time)
                    continue
            raise e
    
    raise Exception(f"√âchec apr√®s {max_retries} tentatives pour le segment {segment_num}")

async def extract_and_update_medecins(analysis_text: str, source_filename: str):
    """Extrait automatiquement les m√©decins de l'analyse et met √† jour la base de donn√©es."""
    try:
        chat = LlmChat(
            api_key=EMERGENT_LLM_KEY,
            session_id=f"extract-medecins-{uuid.uuid4()}",
            system_message="""Tu es un extracteur de donn√©es. Analyse le texte et extrais les informations sur les m√©decins.
R√©ponds UNIQUEMENT en JSON valide. Si aucun m√©decin trouv√©, retourne {"medecins": []}"""
        ).with_model("gemini", "gemini-2.5-flash")
        
        extract_message = UserMessage(
            text=f"""Analyse ce texte et extrait les m√©decins mentionn√©s.

TEXTE:
{analysis_text[:15000]}

Retourne ce JSON:
{{
  "medecins": [
    {{
      "nom": "NOM_MAJUSCULES",
      "prenom": "Pr√©nom",
      "specialite": "sp√©cialit√© ou null",
      "mandataire": "employeur" ou "employe" ou "CNESST" ou "TAT" ou "BEM" ou "inconnu",
      "conclusion_favorable_a": "employeur" ou "employe" ou "neutre"
    }}
  ]
}}"""
        )
        
        response = await chat.send_message(extract_message)
        
        import json
        json_str = response.strip()
        if json_str.startswith("```json"):
            json_str = json_str[7:]
        if json_str.startswith("```"):
            json_str = json_str[3:]
        if json_str.endswith("```"):
            json_str = json_str[:-3]
        
        try:
            data = json.loads(json_str.strip())
        except json.JSONDecodeError:
            logger.warning(f"Impossible de parser le JSON des m√©decins")
            return
        
        if not data.get("medecins"):
            return
        
        for med in data["medecins"]:
            nom = med.get("nom", "").strip().upper()
            prenom = med.get("prenom", "").strip().title()
            
            if not nom or len(nom) < 2:
                continue
            
            existing = await db.medecins.find_one({
                "nom": nom,
                "prenom": {"$regex": f"^{prenom}$", "$options": "i"} if prenom else {"$exists": True}
            })
            
            if existing:
                medecin_id = existing["id"]
                update_data = {"derniere_maj": datetime.now(timezone.utc).isoformat()}
                if med.get("specialite") and not existing.get("specialite"):
                    update_data["specialite"] = med["specialite"]
                await db.medecins.update_one({"id": medecin_id}, {"$set": update_data})
            else:
                medecin_id = str(uuid.uuid4())
                new_medecin = {
                    "id": medecin_id,
                    "nom": nom,
                    "prenom": prenom,
                    "specialite": med.get("specialite"),
                    "adresse": None,
                    "ville": None,
                    "diplomes": None,
                    "decisions_pro_employeur": 0,
                    "decisions_pro_employe": 0,
                    "total_decisions": 0,
                    "pourcentage_pro_employeur": 0.0,
                    "pourcentage_pro_employe": 0.0,
                    "sources": [],
                    "derniere_maj": datetime.now(timezone.utc).isoformat()
                }
                await db.medecins.insert_one(new_medecin)
            
            conclusion = med.get("conclusion_favorable_a", "neutre")
            inc_fields = {"total_decisions": 1}
            if conclusion == "employeur":
                inc_fields["decisions_pro_employeur"] = 1
            elif conclusion == "employe":
                inc_fields["decisions_pro_employe"] = 1
            
            await db.medecins.update_one(
                {"id": medecin_id},
                {"$inc": inc_fields, "$addToSet": {"sources": source_filename}}
            )
            
            medecin_updated = await db.medecins.find_one({"id": medecin_id})
            if medecin_updated and medecin_updated["total_decisions"] > 0:
                total = medecin_updated["total_decisions"]
                pct_employeur = (medecin_updated["decisions_pro_employeur"] / total) * 100
                pct_employe = (medecin_updated["decisions_pro_employe"] / total) * 100
                await db.medecins.update_one(
                    {"id": medecin_id},
                    {"$set": {
                        "pourcentage_pro_employeur": round(pct_employeur, 1),
                        "pourcentage_pro_employe": round(pct_employe, 1)
                    }}
                )
        
        logger.info(f"Extraction termin√©e: {len(data['medecins'])} m√©decin(s)")
        
    except Exception as e:
        logger.error(f"Erreur extraction m√©decins: {str(e)}")

class MultiAnalysisResponse(BaseModel):
    success: bool
    total_files: int
    combined_analysis: str
    anonymized_for_ai: str
    message: str
    files_analyzed: List[str]
    destruction_confirmed: bool = True

@api_router.post("/analyze-multiple", response_model=MultiAnalysisResponse)
async def analyze_multiple_documents(files: List[UploadFile] = File(...), consent_ai_learning: bool = False):
    """Analyse plusieurs documents et retourne un rapport combin√©."""
    
    if len(files) > 10:
        raise HTTPException(status_code=400, detail="Maximum 10 fichiers √† la fois")
    
    # V√©rifier tous les formats
    for f in files:
        if not is_accepted_format(f.filename):
            accepted = ", ".join(ACCEPTED_FORMATS.keys())
            raise HTTPException(status_code=400, detail=f"Format non accept√© pour {f.filename}. Formats accept√©s: {accepted}")
    
    all_analyses = []
    files_analyzed = []
    tmp_paths = []
    
    try:
        for idx, file in enumerate(files, 1):
            contents = await file.read()
            file_size = len(contents)
            
            if file_size > 100 * 1024 * 1024:
                continue  # Skip files over 100 Mo
            
            ext = get_file_extension(file.filename)
            with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp_file:
                tmp_file.write(contents)
                tmp_path = tmp_file.name
                tmp_paths.append(tmp_path)
            
            logger.info(f"Analyse du fichier {idx}/{len(files)}: {file.filename}")
            
            # Analyser le document
            mime_type = ACCEPTED_FORMATS.get(ext, 'application/octet-stream')
            analysis = await analyze_single_file(tmp_path, mime_type, file.filename, idx, len(files))
            all_analyses.append(f"## üìÑ Document {idx}: {file.filename}\n\n{analysis}")
            files_analyzed.append(file.filename)
        
        # Combiner toutes les analyses
        combined = "# üìã RAPPORT D'ANALYSE COMBIN√â - L'√âCLAIREUR\n\n"
        combined += f"**{len(files_analyzed)} document(s) analys√©(s)**\n\n"
        combined += "---\n\n".join(all_analyses)
        
        # Anonymisation
        report_analysis = anonymize_for_report(combined)
        ai_analysis = anonymize_for_ai_learning(combined) if consent_ai_learning else ""
        
        # Destruction s√©curis√©e
        for path in tmp_paths:
            if os.path.exists(path):
                destruction_securisee(path)
        
        return MultiAnalysisResponse(
            success=True,
            total_files=len(files_analyzed),
            combined_analysis=report_analysis,
            anonymized_for_ai=ai_analysis,
            message=f"{len(files_analyzed)} document(s) analys√©(s). Tous les fichiers ont √©t√© d√©truits de mani√®re s√©curis√©e.",
            files_analyzed=files_analyzed,
            destruction_confirmed=True
        )
        
    except Exception as e:
        # Destruction en cas d'erreur
        for path in tmp_paths:
            if os.path.exists(path):
                destruction_securisee(path)
        raise HTTPException(status_code=500, detail=f"Erreur lors de l'analyse: {str(e)}")

async def analyze_single_file(file_path: str, mime_type: str, filename: str, idx: int, total: int) -> str:
    """Analyse un seul fichier avec Gemini."""
    import asyncio
    
    date_analyse = datetime.now(timezone.utc).strftime("%d/%m/%Y √† %H:%M UTC")
    
    for attempt in range(3):
        try:
            chat = LlmChat(
                api_key=EMERGENT_LLM_KEY,
                session_id=f"analysis-{uuid.uuid4()}",
                system_message=SYSTEM_MESSAGE_ANALYSE.replace("{date_analyse}", date_analyse)
            ).with_model("gemini", "gemini-2.5-flash")
            
            file_content = FileContentWithMimeType(
                file_path=file_path,
                mime_type=mime_type
            )
            
            user_message = UserMessage(
                text=f"""Analyse ce document ({filename} - fichier {idx}/{total}) et produis un RAPPORT DE D√âFENSE.

RAPPELS:
1. EXPLIQUE chaque terme m√©dical/technique entre parenth√®ses
2. Indique les num√©ros de pages si possible
3. TABLEAU R√âCAPITULATIF DES CONTRADICTIONS en fin de rapport
4. Inclus le BAR√àME DES INDEMNISATIONS applicable
5. Pr√©pare des QUESTIONS STRAT√âGIQUES pour l'audience TAT

ANONYMISATION - MASQUER uniquement: NAS, RAMQ, Permis, Coordonn√©es bancaires
GARDER EN CLAIR: noms, t√©l√©phones, adresses""",
                file_contents=[file_content]
            )
            
            response = await chat.send_message(user_message)
            return response
            
        except Exception as e:
            if attempt < 2:
                await asyncio.sleep(10)
                continue
            raise e
    
    return f"Erreur lors de l'analyse de {filename}"

# ===== ROUTES =====
@api_router.get("/")
async def root():
    return {"message": "Bienvenue sur L'√âclaireur API", "status": "operational"}

@api_router.get("/health")
async def health_check():
    return {"status": "healthy", "service": "L'√âclaireur"}

# Formats accept√©s
ACCEPTED_FORMATS = {
    '.pdf': 'application/pdf',
    '.doc': 'application/msword',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.jpg': 'image/jpeg',
    '.jpeg': 'image/jpeg',
    '.png': 'image/png',
    '.tiff': 'image/tiff',
    '.tif': 'image/tiff',
    '.bmp': 'image/bmp',
    '.txt': 'text/plain',
    '.rtf': 'application/rtf',
    '.zip': 'application/zip',
    '.rar': 'application/x-rar-compressed',
}

def get_file_extension(filename: str) -> str:
    """Retourne l'extension du fichier en minuscules."""
    return os.path.splitext(filename.lower())[1]

def is_accepted_format(filename: str) -> bool:
    """V√©rifie si le format est accept√©."""
    ext = get_file_extension(filename)
    return ext in ACCEPTED_FORMATS

def extract_pdfs_from_zip(zip_path: str) -> List[str]:
    """Extrait tous les fichiers PDF d'un ZIP et retourne leurs chemins temporaires."""
    extracted_paths = []
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for file_info in zip_ref.infolist():
                if file_info.filename.lower().endswith('.pdf') and not file_info.filename.startswith('__MACOSX'):
                    # Extraire le PDF
                    extracted_name = os.path.basename(file_info.filename)
                    if extracted_name:  # Ignorer les dossiers
                        temp_path = os.path.join(tempfile.gettempdir(), f"extracted_{uuid.uuid4()}_{extracted_name}")
                        with zip_ref.open(file_info) as source, open(temp_path, 'wb') as target:
                            target.write(source.read())
                        extracted_paths.append(temp_path)
                        logger.info(f"PDF extrait du ZIP: {extracted_name}")
        return extracted_paths
    except Exception as e:
        logger.error(f"Erreur extraction ZIP: {str(e)}")
        # Nettoyer en cas d'erreur
        for path in extracted_paths:
            if os.path.exists(path):
                destruction_securisee(path)
        return []

def extract_pdfs_from_rar(rar_path: str) -> List[str]:
    """Extrait tous les fichiers PDF d'un RAR et retourne leurs chemins temporaires."""
    extracted_paths = []
    try:
        with rarfile.RarFile(rar_path, 'r') as rar_ref:
            for file_info in rar_ref.infolist():
                if file_info.filename.lower().endswith('.pdf'):
                    # Extraire le PDF
                    extracted_name = os.path.basename(file_info.filename)
                    if extracted_name:  # Ignorer les dossiers
                        temp_path = os.path.join(tempfile.gettempdir(), f"extracted_{uuid.uuid4()}_{extracted_name}")
                        with rar_ref.open(file_info) as source, open(temp_path, 'wb') as target:
                            target.write(source.read())
                        extracted_paths.append(temp_path)
                        logger.info(f"PDF extrait du RAR: {extracted_name}")
        return extracted_paths
    except Exception as e:
        logger.error(f"Erreur extraction RAR: {str(e)}")
        # Nettoyer en cas d'erreur
        for path in extracted_paths:
            if os.path.exists(path):
                destruction_securisee(path)
        return []

@api_router.post("/analyze", response_model=AnalysisResponse)
async def analyze_document(file: UploadFile = File(...), consent_ai_learning: bool = False):
    """Analyse un document et retourne un rapport de d√©fense."""
    
    if not is_accepted_format(file.filename):
        accepted = ", ".join(ACCEPTED_FORMATS.keys())
        raise HTTPException(status_code=400, detail=f"Format non accept√©. Formats accept√©s: {accepted}")
    
    contents = await file.read()
    file_size = len(contents)
    max_size = 100 * 1024 * 1024  # 100 Mo
    
    if file_size > max_size:
        raise HTTPException(status_code=400, detail="Le fichier d√©passe la limite de 100 Mo")
    
    logger.info(f"Analyse du fichier: {file.filename} ({file_size / (1024*1024):.2f} Mo)")
    
    chunk_paths = []
    tmp_path = None
    extracted_pdfs = []
    
    try:
        # Sauvegarder temporairement
        ext = get_file_extension(file.filename)
        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp_file:
            tmp_file.write(contents)
            tmp_path = tmp_file.name
        
        # Si c'est un ZIP ou RAR, extraire les PDFs
        if ext == '.zip':
            logger.info("Fichier ZIP d√©tect√©, extraction des PDFs...")
            extracted_pdfs = extract_pdfs_from_zip(tmp_path)
            archive_type = "ZIP"
        elif ext == '.rar':
            logger.info("Fichier RAR d√©tect√©, extraction des PDFs...")
            extracted_pdfs = extract_pdfs_from_rar(tmp_path)
            archive_type = "RAR"
        else:
            archive_type = None
        
        if ext in ['.zip', '.rar']:
            if not extracted_pdfs:
                raise HTTPException(status_code=400, detail=f"Aucun fichier PDF trouv√© dans le {archive_type}")
            logger.info(f"{len(extracted_pdfs)} PDF(s) extraits du {archive_type}")
            
            # Analyser chaque PDF extrait
            all_analyses = []
            total_files = len(extracted_pdfs)
            
            for file_idx, pdf_path in enumerate(extracted_pdfs, 1):
                pdf_name = os.path.basename(pdf_path)
                pdf_size = os.path.getsize(pdf_path)
                logger.info(f"Analyse du PDF {file_idx}/{total_files}: {pdf_name}")
                
                # Segmenter si n√©cessaire
                if pdf_size > MAX_CHUNK_SIZE:
                    pdf_chunks = split_pdf_into_chunks(pdf_path, MAX_CHUNK_SIZE)
                else:
                    pdf_chunks = [pdf_path]
                
                chunk_paths.extend(pdf_chunks)
                
                # Analyser les segments de ce PDF
                pdf_analyses = []
                for seg_idx, chunk_path in enumerate(pdf_chunks, 1):
                    analysis = await analyze_pdf_segment(chunk_path, seg_idx, len(pdf_chunks))
                    pdf_analyses.append(analysis)
                
                if len(pdf_chunks) > 1:
                    combined = f"## üìÑ {pdf_name}\n\n" + "\n---\n".join(pdf_analyses)
                else:
                    combined = f"## üìÑ {pdf_name}\n\n{pdf_analyses[0]}"
                
                all_analyses.append(combined)
            
            combined_analysis = f"# üìã ANALYSE DE {total_files} DOCUMENT(S) ({archive_type})\n\n"
            combined_analysis += "\n\n---\n\n".join(all_analyses)
            total_segments = len(chunk_paths)
            
        else:
            # Traitement normal pour les autres fichiers
            # Segmenter si PDF volumineux
            if ext == '.pdf' and file_size > MAX_CHUNK_SIZE:
                logger.info(f"Fichier volumineux, segmentation en cours...")
                chunk_paths = split_pdf_into_chunks(tmp_path, MAX_CHUNK_SIZE)
            else:
                chunk_paths = [tmp_path]
            
            # Analyser chaque segment
            all_analyses = []
            total_segments = len(chunk_paths)
            
            for i, chunk_path in enumerate(chunk_paths, 1):
                logger.info(f"Analyse du segment {i}/{total_segments}...")
                segment_analysis = await analyze_pdf_segment(chunk_path, i, total_segments)
                all_analyses.append(segment_analysis)
            
            # Combiner les analyses
            if total_segments > 1:
                combined_analysis = f"üìÑ **ANALYSE COMPL√àTE DU DOCUMENT** ({total_segments} segments)\n\n"
                combined_analysis += "---\n\n".join([
                    f"### Segment {i+1}/{total_segments}\n\n{analysis}" 
                    for i, analysis in enumerate(all_analyses)
                ])
            else:
                combined_analysis = all_analyses[0]
        
        # Anonymisation pour le rapport (l√©g√®re)
        report_analysis = anonymize_for_report(combined_analysis)
        
        # Anonymisation compl√®te pour l'IA (si consentement)
        ai_analysis = ""
        if consent_ai_learning:
            ai_analysis = anonymize_for_ai_learning(combined_analysis)
            logger.info("Version anonymis√©e cr√©√©e pour apprentissage IA")
        
        # Extraire les m√©decins
        await extract_and_update_medecins(combined_analysis, file.filename)
        
        # DESTRUCTION S√âCURIS√âE DOD 5220.22-M
        destruction_success = True
        for chunk_path in chunk_paths:
            if os.path.exists(chunk_path):
                destruction_success = destruction_securisee(chunk_path) and destruction_success
        # D√©truire les PDFs extraits du ZIP
        for pdf_path in extracted_pdfs:
            if os.path.exists(pdf_path):
                destruction_success = destruction_securisee(pdf_path) and destruction_success
        if tmp_path and os.path.exists(tmp_path) and tmp_path not in chunk_paths:
            destruction_success = destruction_securisee(tmp_path) and destruction_success
        
        logger.info(f"Analyse termin√©e pour: {file.filename} - Destruction s√©curis√©e: {destruction_success}")
        
        # NE PAS sauvegarder en base (aucune copie gard√©e)
        
        return AnalysisResponse(
            success=True,
            filename=file.filename,
            file_size=file_size,
            analysis=report_analysis,
            anonymized_for_ai=ai_analysis,
            message=f"Analyse termin√©e ({total_segments} segment{'s' if total_segments > 1 else ''}). Document d√©truit de mani√®re s√©curis√©e.",
            segments_analyzed=total_segments,
            destruction_confirmed=destruction_success
        )
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse: {str(e)}")
        # Destruction s√©curis√©e en cas d'erreur
        for chunk_path in chunk_paths:
            if os.path.exists(chunk_path):
                destruction_securisee(chunk_path)
        for pdf_path in extracted_pdfs:
            if os.path.exists(pdf_path):
                destruction_securisee(pdf_path)
        if tmp_path and os.path.exists(tmp_path):
            destruction_securisee(tmp_path)
        raise HTTPException(status_code=500, detail=f"Erreur lors de l'analyse: {str(e)}")

# ===== COMPTEUR VISITEURS =====
@api_router.get("/stats/visitors")
async def get_visitor_count():
    stats = await db.stats.find_one({"type": "visitors"})
    if not stats:
        await db.stats.insert_one({"type": "visitors", "count": 0})
        return {"count": 0}
    return {"count": stats.get("count", 0)}

@api_router.post("/stats/visitors/increment")
async def increment_visitor_count():
    await db.stats.update_one({"type": "visitors"}, {"$inc": {"count": 1}}, upsert=True)
    stats = await db.stats.find_one({"type": "visitors"})
    return {"count": stats.get("count", 0)}

# ===== D√âCOUPAGE PDF AUTOMATIQUE =====
SPLIT_TARGET_SIZE = 15 * 1024 * 1024  # 15 Mo par partie (sous la limite Gemini de 20 Mo)

@api_router.post("/split-pdf")
async def split_pdf_for_download(file: UploadFile = File(...)):
    """
    D√©coupe un gros PDF en plusieurs parties t√©l√©chargeables.
    Retourne un fichier ZIP contenant toutes les parties.
    """
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Seuls les fichiers PDF sont accept√©s")
    
    contents = await file.read()
    file_size = len(contents)
    
    # V√©rifier si le fichier n√©cessite un d√©coupage
    if file_size <= SPLIT_TARGET_SIZE:
        raise HTTPException(status_code=400, detail="Ce fichier est assez petit pour √™tre analys√© directement (< 15 Mo)")
    
    tmp_path = None
    try:
        # Sauvegarder temporairement
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(contents)
            tmp_path = tmp_file.name
        
        reader = PdfReader(tmp_path)
        total_pages = len(reader.pages)
        
        if total_pages == 0:
            raise HTTPException(status_code=400, detail="Le PDF semble vide")
        
        # Calculer le nombre de pages par partie
        avg_page_size = file_size / total_pages
        pages_per_part = max(1, int(SPLIT_TARGET_SIZE / avg_page_size))
        num_parts = math.ceil(total_pages / pages_per_part)
        
        logger.info(f"D√©coupage de {file.filename}: {total_pages} pages en {num_parts} parties de ~{pages_per_part} pages")
        
        # Cr√©er le ZIP en m√©moire
        zip_buffer = io.BytesIO()
        base_filename = file.filename.rsplit('.', 1)[0]
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for i in range(num_parts):
                writer = PdfWriter()
                start_page = i * pages_per_part
                end_page = min((i + 1) * pages_per_part, total_pages)
                
                for page_num in range(start_page, end_page):
                    writer.add_page(reader.pages[page_num])
                
                # √âcrire la partie dans un buffer
                part_buffer = io.BytesIO()
                writer.write(part_buffer)
                part_buffer.seek(0)
                
                # Ajouter au ZIP
                part_filename = f"{base_filename}_partie_{i+1}_sur_{num_parts}.pdf"
                zip_file.writestr(part_filename, part_buffer.getvalue())
                
                logger.info(f"Partie {i+1}/{num_parts} cr√©√©e: pages {start_page+1}-{end_page}")
        
        # Nettoyer le fichier temporaire
        if tmp_path and os.path.exists(tmp_path):
            destruction_securisee(tmp_path)
        
        # Retourner le ZIP
        zip_buffer.seek(0)
        zip_filename = f"{base_filename}_decoupe_{num_parts}_parties.zip"
        
        return StreamingResponse(
            zip_buffer,
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename={zip_filename}",
                "X-Parts-Count": str(num_parts),
                "X-Pages-Per-Part": str(pages_per_part)
            }
        )
        
    except Exception as e:
        if tmp_path and os.path.exists(tmp_path):
            destruction_securisee(tmp_path)
        logger.error(f"Erreur d√©coupage PDF: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors du d√©coupage: {str(e)}")

# ===== T√âMOIGNAGES =====
class TestimonialCreate(BaseModel):
    name: str = Field(..., min_length=2, max_length=50)
    message: str = Field(..., min_length=10, max_length=500)
    rating: int = Field(..., ge=1, le=5)

@api_router.post("/testimonials")
async def create_testimonial(testimonial: TestimonialCreate):
    est_valide, msg = moderer_contenu(testimonial.message)
    if not est_valide:
        raise HTTPException(status_code=400, detail=msg)
    
    doc = {
        "id": str(uuid.uuid4()),
        "name": testimonial.name,
        "message": testimonial.message,
        "rating": testimonial.rating,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "approved": True
    }
    await db.testimonials.insert_one(doc)
    return {"message": "T√©moignage soumis avec succ√®s", "id": doc["id"]}

@api_router.get("/testimonials")
async def get_testimonials():
    testimonials = await db.testimonials.find({"approved": True}, {"_id": 0}).sort("timestamp", -1).to_list(20)
    return testimonials

# ===== M√âDECINS =====
DISCLAIMER_MEDECIN = """
‚öñÔ∏è AVIS IMPORTANT - CLAUSE DE NON-RESPONSABILIT√â

Les statistiques sont compil√©es √† partir de d√©cisions publiques du TAT et autres sources publiques.
Ces informations sont fournies √Ä TITRE INFORMATIF SEULEMENT.

‚Ä¢ Ces donn√©es ne constituent PAS une accusation de partialit√©.
‚Ä¢ Les pourcentages refl√®tent uniquement les d√©cisions document√©es.
‚Ä¢ Chaque dossier est unique.

Pour v√©rifier le dossier disciplinaire d'un m√©decin:
https://www.cmq.org/fr/proteger-le-public/suivre-dossier-disciplinaire/decisions-disciplinaires
"""

@api_router.get("/medecins")
async def get_medecins():
    medecins = await db.medecins.find({}, {"_id": 0}).sort("nom", 1).to_list(500)
    return {"disclaimer": DISCLAIMER_MEDECIN, "medecins": medecins}

@api_router.get("/medecins/{medecin_id}")
async def get_medecin(medecin_id: str):
    medecin = await db.medecins.find_one({"id": medecin_id}, {"_id": 0})
    if not medecin:
        raise HTTPException(status_code=404, detail="M√©decin non trouv√©")
    
    contributions = await db.contributions.find(
        {"medecin_id": medecin_id, "approved": True}, {"_id": 0}
    ).sort("timestamp", -1).to_list(50)
    
    return {"disclaimer": DISCLAIMER_MEDECIN, "medecin": medecin, "contributions": contributions}

@api_router.get("/medecins/search/{nom}")
async def search_medecin(nom: str):
    medecins = await db.medecins.find(
        {"$or": [
            {"nom": {"$regex": nom, "$options": "i"}},
            {"prenom": {"$regex": nom, "$options": "i"}}
        ]}, {"_id": 0}
    ).to_list(20)
    return {"disclaimer": DISCLAIMER_MEDECIN, "medecins": medecins}

# ===== CONTRIBUTIONS =====
@api_router.post("/contributions")
async def create_contribution(contribution: ContributionCreate):
    est_valide, msg = moderer_contenu(contribution.description)
    if not est_valide:
        raise HTTPException(status_code=400, detail=msg)
    
    if contribution.source_reference:
        est_valide, msg = moderer_contenu(contribution.source_reference)
        if not est_valide:
            raise HTTPException(status_code=400, detail=msg)
    
    medecin = await db.medecins.find_one({
        "nom": {"$regex": f"^{contribution.medecin_nom}$", "$options": "i"},
        "prenom": {"$regex": f"^{contribution.medecin_prenom}$", "$options": "i"}
    })
    
    if not medecin:
        medecin_id = str(uuid.uuid4())
        medecin = {
            "id": medecin_id,
            "nom": contribution.medecin_nom.upper(),
            "prenom": contribution.medecin_prenom.title(),
            "specialite": None, "adresse": None, "ville": None, "diplomes": None,
            "decisions_pro_employeur": 0, "decisions_pro_employe": 0, "total_decisions": 0,
            "pourcentage_pro_employeur": 0.0, "pourcentage_pro_employe": 0.0,
            "sources": [], "derniere_maj": datetime.now(timezone.utc).isoformat()
        }
        await db.medecins.insert_one(medecin)
    else:
        medecin_id = medecin["id"]
    
    contribution_doc = {
        "id": str(uuid.uuid4()),
        "medecin_id": medecin_id,
        "medecin_nom": contribution.medecin_nom.upper(),
        "medecin_prenom": contribution.medecin_prenom.title(),
        "type_contribution": contribution.type_contribution,
        "description": contribution.description,
        "source_reference": contribution.source_reference,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "approved": True
    }
    await db.contributions.insert_one(contribution_doc)
    
    inc_fields = {"total_decisions": 1}
    if contribution.type_contribution == "pro_employeur":
        inc_fields["decisions_pro_employeur"] = 1
    elif contribution.type_contribution == "pro_employe":
        inc_fields["decisions_pro_employe"] = 1
    
    update_ops = {"$set": {"derniere_maj": datetime.now(timezone.utc).isoformat()}, "$inc": inc_fields}
    if contribution.source_reference:
        update_ops["$addToSet"] = {"sources": contribution.source_reference}
    
    await db.medecins.update_one({"id": medecin_id}, update_ops)
    
    medecin_updated = await db.medecins.find_one({"id": medecin_id})
    if medecin_updated and medecin_updated["total_decisions"] > 0:
        pct_employeur = (medecin_updated["decisions_pro_employeur"] / medecin_updated["total_decisions"]) * 100
        pct_employe = (medecin_updated["decisions_pro_employe"] / medecin_updated["total_decisions"]) * 100
        await db.medecins.update_one({"id": medecin_id}, {"$set": {
            "pourcentage_pro_employeur": round(pct_employeur, 1),
            "pourcentage_pro_employe": round(pct_employe, 1)
        }})
    
    return {
        "message": "Contribution enregistr√©e avec succ√®s!",
        "id": contribution_doc["id"],
        "disclaimer": DISCLAIMER_MEDECIN
    }

@api_router.get("/contributions")
async def get_contributions():
    contributions = await db.contributions.find({"approved": True}, {"_id": 0}).sort("timestamp", -1).to_list(100)
    return contributions

@api_router.get("/stats/medecins")
async def get_medecins_stats():
    total_medecins = await db.medecins.count_documents({})
    total_contributions = await db.contributions.count_documents({"approved": True})
    top_medecins = await db.medecins.find({"total_decisions": {"$gt": 0}}, {"_id": 0}).sort("total_decisions", -1).to_list(10)
    
    return {
        "disclaimer": DISCLAIMER_MEDECIN,
        "total_medecins_documentes": total_medecins,
        "total_contributions": total_contributions,
        "top_medecins_documentes": top_medecins
    }

# ===== NETTOYAGE =====
@api_router.delete("/nettoyer")
async def nettoyer_fichiers_temporaires():
    """Nettoie tous les fichiers temporaires de mani√®re s√©curis√©e."""
    fichiers_supprimes = 0
    for nom_fichier in os.listdir(UPLOAD_DIR):
        if nom_fichier.endswith('.pdf'):
            chemin = os.path.join(UPLOAD_DIR, nom_fichier)
            if os.path.isfile(chemin):
                destruction_securisee(chemin)
                fichiers_supprimes += 1
    return {"status": "nettoy√©", "message": f"{fichiers_supprimes} fichier(s) supprim√©(s)"}

# Include router and CORS
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=os.environ.get('CORS_ORIGINS', '*').split(','),
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()
